{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\.conda\\envs\\myenv\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BTCUSDT = pd.read_csv(\"BTCUSDT.csv\" , on_bad_lines='skip')\n",
    "df_USDIRT = pd.read_csv(\"USDIRT.csv\")\n",
    "df_Wallex_USDIRT = pd.read_csv(\"Wallex_USDIRT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BTCUSDT_test = pd.read_csv(\"BTC_TEST.csv\" , on_bad_lines='skip')\n",
    "df_USDIRT_test = pd.read_csv(\"DOLLAR_TEST.csv\")\n",
    "df_Wallex_USDIRT_test = pd.read_csv(\"TETHER_TEST.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BTCUSDT = df_BTCUSDT.drop(\"Unnamed: 0\" , axis = 1)\n",
    "df_BTCUSDT = df_BTCUSDT.drop(\"Datetime\" , axis = 1)\n",
    "df_USDIRT = df_USDIRT.drop(\"Unnamed: 0\" , axis = 1)\n",
    "df_Wallex_USDIRT = df_Wallex_USDIRT.drop(\"Unnamed: 0\" , axis = 1)\n",
    "df_Wallex_USDIRT = df_Wallex_USDIRT.drop(\"Datetime\" , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_BTCUSDT_test = df_BTCUSDT_test.drop(\"Unnamed: 0\" , axis = 1)\n",
    "df_BTCUSDT_test = df_BTCUSDT_test.drop(\"Datetime\" , axis = 1)\n",
    "#df_USDIRT_test = df_USDIRT_test.drop(\"Unnamed: 0\" , axis = 1)\n",
    "#df_Wallex_USDIRT_test = df_Wallex_USDIRT_test.drop(\"Unnamed: 0\" , axis = 1)\n",
    "df_Wallex_USDIRT_test = df_Wallex_USDIRT_test.drop(\"Datetime\" , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_next = []\n",
    "before = 0 \n",
    "for i in range(len( df_Wallex_USDIRT[\"Close\"])) :\n",
    "  if i != 0  :\n",
    "    if ((df_Wallex_USDIRT[\"Close\"].iloc[i]) > (df_Wallex_USDIRT[\"Close\"].iloc[before])):\n",
    "      predict_next.append(1)\n",
    "    else : \n",
    "      predict_next.append(0)\n",
    "  before = i \n",
    "predict_next.append(0)\n",
    "predict_next = pd.DataFrame(predict_next)\n",
    "df_Wallex_USDIRT = np.concatenate([df_Wallex_USDIRT , predict_next] , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Wallex_USDIRT = pd.DataFrame(df_Wallex_USDIRT , columns=[ \"Open\" , \"Close\" , \"High\" , \"Low\" , \"Volume\" , \"predict_next\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_next = []\n",
    "before = 0 \n",
    "for i in range(len( df_Wallex_USDIRT_test[\"Close\"])) :\n",
    "  if i != 0  :\n",
    "    if ((df_Wallex_USDIRT_test[\"Close\"].iloc[i]) > (df_Wallex_USDIRT_test[\"Close\"].iloc[before])):\n",
    "      predict_next.append(1)\n",
    "    else : \n",
    "      predict_next.append(0)\n",
    "  before = i \n",
    "predict_next.append(0)\n",
    "predict_next = pd.DataFrame(predict_next)\n",
    "df_Wallex_USDIRT_test = np.concatenate([df_Wallex_USDIRT_test , predict_next] , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Wallex_USDIRT_test = pd.DataFrame(df_Wallex_USDIRT_test , columns=[ \"Open\" , \"Close\" , \"High\" , \"Low\" , \"Volume\" , \"predict_next\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_usdirt = df_USDIRT.CLOSE.mean()\n",
    "df_USDIRT[\"CLOSE\"] = df_USDIRT[\"CLOSE\"].fillna(avg_usdirt)\n",
    "avg_BTCUSDT = df_BTCUSDT[\"Close\"].mean()\n",
    "df_BTCUSDT.Close = df_BTCUSDT.Close.fillna(avg_BTCUSDT)\n",
    "df_Wallex_USDIRT.rename(columns = {\"Open\" : \"Open_W\" , \"Close\" : \"Close_W\" , \"High\" : \"High_W\" , \"Low\" : \"Low_W\" , \"Volume\" : \"Volume_W\"} , inplace=True)\n",
    "df_BTCUSDT.rename(columns = {\"Open\" : \"Open_BD\" , \"Close\" : \"Close_BD\" , \"High\" : \"High_BD\" , \"Low\" : \"Low_BD\" } , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_usdirt = df_USDIRT_test.CLOSE.mean()\n",
    "df_USDIRT_test[\"CLOSE\"] = df_USDIRT_test[\"CLOSE\"].fillna(avg_usdirt)\n",
    "avg_BTCUSDT = df_BTCUSDT_test[\"Close\"].mean()\n",
    "df_BTCUSDT_test.Close = df_BTCUSDT_test.Close.fillna(avg_BTCUSDT)\n",
    "df_Wallex_USDIRT_test.rename(columns = {\"Open\" : \"Open_W\" , \"Close\" : \"Close_W\" , \"High\" : \"High_W\" , \"Low\" : \"Low_W\" , \"Volume\" : \"Volume_W\"} , inplace=True)\n",
    "df_BTCUSDT_test.rename(columns = {\"Open\" : \"Open_BD\" , \"Close\" : \"Close_BD\" , \"High\" : \"High_BD\" , \"Low\" : \"Low_BD\" } , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean all ? values\n",
    "check = df_Wallex_USDIRT.replace('?' , 0)\n",
    "check[\"Volume_W\"] =  check[\"Volume_W\"].astype(float)\n",
    "test = df_Wallex_USDIRT.replace(\"?\" , method=\"bfill\")\n",
    "test[\"Volume_W\"] =  test[\"Volume_W\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean all ? values\n",
    "check = df_Wallex_USDIRT_test.replace('?' , 0)\n",
    "check[\"Volume_W\"] =  check[\"Volume_W\"].astype(float)\n",
    "test_test = df_Wallex_USDIRT_test.replace(\"?\" , method=\"bfill\")\n",
    "test_test[\"Volume_W\"] =  test_test[\"Volume_W\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all not importants\n",
    "df_USDIRT.drop(\"VOL\", axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all not importants\n",
    "df_USDIRT_test.drop(\"VOL\", axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all =[df_USDIRT , df_BTCUSDT , test]\n",
    "df_all = pd.concat(df_all , axis=1 , join = \"inner\" , copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_test =[df_USDIRT_test , df_BTCUSDT_test , test_test]\n",
    "df_all_test = pd.concat(df_all_test , axis=1 , join = \"inner\" , copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding features\n",
    "# add $ / tether\n",
    "tether_DivBy_dollor = []\n",
    "for i in range(len(df_all)) : \n",
    "  ratio =  df_all[\"CLOSE\"].iloc[i] /  df_all[\"Close_W\"].iloc[i]\n",
    "  tether_DivBy_dollor.append(ratio)\n",
    "tether_DivBy_dollor = pd.DataFrame(tether_DivBy_dollor)\n",
    "# add distance between tether and $ with look to the tether\n",
    "distance_DivBy_dollor = []\n",
    "for i in range(len(df_all)) : \n",
    "  distance = df_all[\"CLOSE\"].iloc[i] - df_all[\"Close_W\"].iloc[i]\n",
    "  ratio = distance / df_all[\"Close_W\"].iloc[i]\n",
    "  distance_DivBy_dollor.append(ratio) # tether i mean =)) \n",
    "distance_DivBy_dollor = pd.DataFrame(distance_DivBy_dollor)\n",
    "# add randeman in 10 min before\n",
    "randeman_10 = []\n",
    "for i in range(len(df_all)) : \n",
    "  sum = 0\n",
    "  for j in range(0,11) :\n",
    "    if (i - j) >= 0 :\n",
    "      sum = sum + df_all[\"predict_next\"].iloc[i-j]\n",
    "  randeman_10.append(sum / 10 )\n",
    "randeman_10 = pd.DataFrame(randeman_10)\n",
    "# add randeman in 30 min before\n",
    "randeman_30 = []\n",
    "for i in range(len(df_all)) : \n",
    "  sum = 0\n",
    "  for j in range(0,31) :\n",
    "    if (i - j) >= 0 :\n",
    "      sum = sum + df_all[\"predict_next\"].iloc[i-j]\n",
    "  randeman_30.append(sum / 30 )\n",
    "randeman_30 = pd.DataFrame(randeman_30)\n",
    "# add randeman in 5 min before\n",
    "randeman_5 = []\n",
    "for i in range(len(df_all)) : \n",
    "  sum = 0\n",
    "  for j in range(9,6) :\n",
    "    if (i - j) >= 0 :\n",
    "      sum = sum + df_all[\"predict_next\"].iloc[i-j]\n",
    "  randeman_5.append(sum / 5 )\n",
    "randeman_5 = pd.DataFrame(randeman_5)\n",
    "# add randeman in 60 min before\n",
    "randeman_60 = []\n",
    "for i in range(len(df_all)) : \n",
    "  sum = 0\n",
    "  for j in range(0,61) :\n",
    "    if (i - j) >= 0 :\n",
    "      sum = sum + df_all[\"predict_next\"].iloc[i-j]\n",
    "  randeman_60.append(sum / 60 )\n",
    "randeman_60 = pd.DataFrame(randeman_60)\n",
    "mode_randeman_60 = randeman_60.mode()\n",
    "## balancing first values\n",
    "for i in range(0,61) :\n",
    "  randeman_60.iloc[i] = randeman_60.iloc[i] + mode_randeman_60  \n",
    "# mean of valume in 1 hour ago\n",
    "mean_valume = []\n",
    "for i in range(len(df_all)) :\n",
    "  avg_valume = 0\n",
    "  for j in range(0,61) :\n",
    "    if (i - j) >= 0 :\n",
    "      avg_valume = avg_valume + df_all[\"Volume_W\"].iloc[i-j]\n",
    "  mean_valume.append(avg_valume/60)\n",
    "mean_valume = pd.DataFrame(mean_valume)\n",
    "mean_mean_volume = mean_valume.mean()\n",
    "for i in range(0,61) :\n",
    "  mean_valume.iloc[i] = mean_valume.iloc[i] + mean_mean_volume  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add all features\n",
    "df_all = [df_all , tether_DivBy_dollor , distance_DivBy_dollor , randeman_5 , randeman_10 , randeman_30 , randeman_60 , mean_valume ]\n",
    "df_all = pd.concat(df_all , axis=1 )\n",
    "# renaming names\n",
    "df_all.columns.values[17] = \"tether_DivBy_dollor\"\n",
    "df_all.columns.values[18] = \"distance_DivBy_dollor\"\n",
    "df_all.columns.values[19] = \"randeman_5\"\n",
    "df_all.columns.values[20] = \"randeman_10\"\n",
    "df_all.columns.values[21] = \"randeman_30\"\n",
    "df_all.columns.values[22] = \"randeman_60\"\n",
    "df_all.columns.values[23] = \"mean_valume\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding features\n",
    "# add $ / tether\n",
    "tether_DivBy_dollor = []\n",
    "for i in range(len(df_all_test)) : \n",
    "  ratio =  df_all_test[\"CLOSE\"].iloc[i] /  df_all_test[\"Close_W\"].iloc[i]\n",
    "  tether_DivBy_dollor.append(ratio)\n",
    "tether_DivBy_dollor = pd.DataFrame(tether_DivBy_dollor)\n",
    "# add distance between tether and $ with look to the tether\n",
    "distance_DivBy_dollor = []\n",
    "for i in range(len(df_all_test)) : \n",
    "  distance = df_all_test[\"CLOSE\"].iloc[i] - df_all_test[\"Close_W\"].iloc[i]\n",
    "  ratio = distance / df_all_test[\"Close_W\"].iloc[i]\n",
    "  distance_DivBy_dollor.append(ratio) # tether i mean =)) \n",
    "distance_DivBy_dollor = pd.DataFrame(distance_DivBy_dollor)\n",
    "# yadam raft in feature ro nemishe estefade kard chon predict next yani javab ro midone ! \n",
    "# add randeman in 10 min before\n",
    "randeman_10 = []\n",
    "for i in range(len(df_all_test)) : \n",
    "  sum = 0\n",
    "  for j in range(0,11) :\n",
    "    if (i - j) >= 0 :\n",
    "      sum = sum + df_all_test[\"predict_next\"].iloc[i-j]\n",
    "  randeman_10.append(sum / 10 )\n",
    "randeman_10 = pd.DataFrame(randeman_10)\n",
    "# add randeman in 30 min before\n",
    "randeman_30 = []\n",
    "for i in range(len(df_all_test)) : \n",
    "  sum = 0\n",
    "  for j in range(0,31) :\n",
    "    if (i - j) >= 0 :\n",
    "      sum = sum + df_all_test[\"predict_next\"].iloc[i-j]\n",
    "  randeman_30.append(sum / 30 )\n",
    "randeman_30 = pd.DataFrame(randeman_30)\n",
    "# add randeman in 5 min before\n",
    "randeman_5 = []\n",
    "for i in range(len(df_all_test)) : \n",
    "  sum = 0\n",
    "  for j in range(9,6) :\n",
    "    if (i - j) >= 0 :\n",
    "      sum = sum + df_all_test[\"predict_next\"].iloc[i-j]\n",
    "  randeman_5.append(sum / 5 )\n",
    "randeman_5 = pd.DataFrame(randeman_5)\n",
    "# add randeman in 60 min before\n",
    "randeman_60 = []\n",
    "for i in range(len(df_all_test)) : \n",
    "  sum = 0\n",
    "  for j in range(0,61) :\n",
    "    if (i - j) >= 0 :\n",
    "      sum = sum + df_all_test[\"predict_next\"].iloc[i-j]\n",
    "  randeman_60.append(sum / 60 )\n",
    "randeman_60 = pd.DataFrame(randeman_60)\n",
    "mode_randeman_60 = randeman_60.mode()\n",
    "## balancing first values\n",
    "for i in range(0,61) :\n",
    "  randeman_60.iloc[i] = randeman_60.iloc[i] + mode_randeman_60  \n",
    "#mean of valume in 1 hour ago\n",
    "mean_valume = []\n",
    "for i in range(len(df_all_test)) :\n",
    "  avg_valume = 0\n",
    "  for j in range(0,61) :\n",
    "    if (i - j) >= 0 :\n",
    "      avg_valume = avg_valume + df_all_test[\"Volume_W\"].iloc[i-j]\n",
    "  mean_valume.append(avg_valume/60)\n",
    "mean_valume = pd.DataFrame(mean_valume)\n",
    "mean_mean_volume = mean_valume.mean()\n",
    "for i in range(0,61) :\n",
    "  mean_valume.iloc[i] = mean_valume.iloc[i] + mean_mean_volume  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add all features\n",
    "df_all_test = [df_all , tether_DivBy_dollor , distance_DivBy_dollor , randeman_5 , randeman_10 , randeman_30 , randeman_60 , mean_valume  ]\n",
    "df_all_test = pd.concat(df_all_test , axis=1 )\n",
    "# renaming names\n",
    "df_all_test.columns.values[17] = \"tether_DivBy_dollor\"\n",
    "df_all_test.columns.values[18] = \"distance_DivBy_dollor\"\n",
    "df_all_test.columns.values[19] = \"randeman_5\"\n",
    "df_all_test.columns.values[20] = \"randeman_10\"\n",
    "df_all_test.columns.values[21] = \"randeman_30\"\n",
    "df_all_test.columns.values[22] = \"randeman_60\"\n",
    "df_all_test.columns.values[23] = \"mean_valume\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add financial features like cci \n",
    "# first : add cci for tether to rial\n",
    "# cci function \n",
    "def CCI(df, ndays): \n",
    "  df['TP'] = (df['High_W'] + df['Low_W'] + df['Close_W']) / 3 \n",
    "  df['sma'] = df['TP'].rolling(ndays).mean()\n",
    "  df['mad'] = df['TP'].rolling(ndays).apply(lambda x: pd.Series(x).mad())\n",
    "  df['CCI'] = (df['TP'] - df['sma']) / (0.015 * df['mad']) \n",
    "  df[\"CCI\"] = df[\"CCI\"].fillna(df[\"CCI\"].mean())\n",
    "  df['mad'] = df['mad'].fillna(df['mad'].mean())\n",
    "  df['sma'] = df['sma'].fillna(df['sma'].mean())\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cci function BD\n",
    "def CCIBD(df, ndays): \n",
    "  df['TPBD'] = (df['High_BD'] + df['Low_BD'] + df['Close_BD']) / 3 \n",
    "  df['smaBD'] = df['TPBD'].rolling(ndays).mean()\n",
    "  df['madBD'] = df['TPBD'].rolling(ndays).apply(lambda x: pd.Series(x).mad())\n",
    "  df['CCIBD'] = (df['TPBD'] - df['smaBD']) / (0.015 * df['madBD']) \n",
    "  df[\"CCIBD\"] = df[\"CCIBD\"].fillna(df[\"CCIBD\"].mean())\n",
    "  df['madBD'] = df['madBD'].fillna(df['madBD'].mean())\n",
    "  df['smaBD'] = df['smaBD'].fillna(df['smaBD'].mean())\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cci function USD\n",
    "def CCIUSD(df, ndays): \n",
    "  df['TPUSD'] = (df['HIGH'] + df['LOW'] + df['CLOSE']) / 3 \n",
    "  df['smaUSD'] = df['TPUSD'].rolling(ndays).mean()\n",
    "  df['madUSD'] = df['TPUSD'].rolling(ndays).apply(lambda x: pd.Series(x).mad())\n",
    "  df['CCIUSD'] = (df['TPUSD'] - df['smaUSD']) / (0.015 * df['madUSD']) \n",
    "  df[\"CCIUSD\"] = df[\"CCIUSD\"].fillna(df[\"CCIUSD\"].mean())\n",
    "  df['madUSD'] = df['madUSD'].fillna(df['madUSD'].mean())\n",
    "  df['smaUSD'] = df['smaUSD'].fillna(df['smaUSD'].mean())\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating strategy \n",
    "def implement_cci_strategy(prices , cci):\n",
    "    cci_signal = []\n",
    "    signal = 0\n",
    "    \n",
    "    lower_band = (-150)\n",
    "    upper_band = 150\n",
    "    cci_signal.append(0)\n",
    "    for i in range(len(prices)):\n",
    "      if i != 0 :\n",
    "        if cci[i-1] > lower_band and cci[i] < lower_band:\n",
    "            if signal != 1:\n",
    "                signal = 1\n",
    "                cci_signal.append(signal)\n",
    "            else:\n",
    "                cci_signal.append(0)\n",
    "\n",
    "        elif cci[i-1] < upper_band and cci[i] > upper_band:\n",
    "            if signal != -1:\n",
    "                signal = -1\n",
    "                cci_signal.append(signal)\n",
    "            else:\n",
    "                cci_signal.append(0) \n",
    "        else:\n",
    "            cci_signal.append(0)\n",
    "            \n",
    "    return cci_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the cci things Wallex\n",
    "df_all = CCI(df_all , 60)\n",
    "cci_signals = implement_cci_strategy(df_all[\"Close_W\"] , df_all[\"CCI\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the cci things usd\n",
    "df_all = CCIUSD(df_all , 60)\n",
    "cci_signals_USD = implement_cci_strategy(df_all[\"CLOSE\"] , df_all[\"CCIUSD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the cci things Wallex BD\n",
    "df_all = CCIBD(df_all , 60)\n",
    "cci_signals_BD = implement_cci_strategy(df_all[\"Close_BD\"] , df_all[\"CCIBD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[\"predict_cci\"] = cci_signals\n",
    "df_all[\"predict_cci_BD\"] = cci_signals_BD\n",
    "df_all[\"predict_cci_USD\"] = cci_signals_USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the cci things Wallex\n",
    "df_all_test = CCI(df_all_test , 60)\n",
    "cci_signals = implement_cci_strategy(df_all_test[\"Close_W\"] , df_all_test[\"CCI\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the cci things Wallex BD\n",
    "df_all_test = CCIBD(df_all_test , 60)\n",
    "cci_signals_BD = implement_cci_strategy(df_all_test[\"Close_BD\"] , df_all_test[\"CCIBD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the cci things usd\n",
    "df_all_test = CCIUSD(df_all_test , 60)\n",
    "cci_signals_USD = implement_cci_strategy(df_all_test[\"CLOSE\"] , df_all_test[\"CCIUSD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_test[\"predict_cci\"] = cci_signals\n",
    "df_all_test[\"predict_cci_BD\"] = cci_signals_BD\n",
    "df_all_test[\"predict_cci_USD\"] = cci_signals_USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add rsi feateures\n",
    "def rsi(df, periods = 14, ema = True):\n",
    "\n",
    "    close_delta = df.diff()\n",
    "\n",
    "    # Make two series: one for lower closes and one for higher closes\n",
    "    up = close_delta.clip(lower=0)\n",
    "    down = -1 * close_delta.clip(upper=0)\n",
    "    \n",
    "    if ema == True:\n",
    "\t    # Use exponential moving average\n",
    "        ma_up = up.ewm(com = periods - 1, adjust=True, min_periods = periods).mean()\n",
    "        ma_down = down.ewm(com = periods - 1, adjust=True, min_periods = periods).mean()\n",
    "    else:\n",
    "        # Use simple moving average\n",
    "        ma_up = up.rolling(window = periods, adjust=False).mean()\n",
    "        ma_down = down.rolling(window = periods, adjust=False).mean()\n",
    "        \n",
    "    rsi = ma_up / ma_down\n",
    "    rsi = 100 - (100/(1 + rsi))\n",
    "    return rsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsi_factor = rsi (df_all[\"Close_W\"])\n",
    "rsi_factor_BD = rsi (df_all[\"Close_BD\"])\n",
    "rsi_factor_USD = rsi (df_all[\"CLOSE\"])\n",
    "\n",
    "df_all[\"rsi_factor\"] = rsi_factor\n",
    "df_all[\"rsi_factor_BD\"] = rsi_factor_BD\n",
    "df_all[\"rsi_factor_USD\"] = rsi_factor_USD\n",
    "\n",
    "df_all[\"rsi_factor\"] = df_all[\"rsi_factor\"].fillna(df_all[\"rsi_factor\"].mean())\n",
    "df_all[\"rsi_factor_BD\"] = df_all[\"rsi_factor_BD\"].fillna(df_all[\"rsi_factor_BD\"].mean())\n",
    "df_all[\"rsi_factor_USD\"] = df_all[\"rsi_factor_USD\"].fillna(df_all[\"rsi_factor_USD\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsi_factor = rsi (df_all_test[\"Close_W\"])\n",
    "rsi_factor_BD = rsi (df_all_test[\"Close_BD\"])\n",
    "rsi_factor_USD = rsi (df_all_test[\"CLOSE\"])\n",
    "\n",
    "df_all_test[\"rsi_factor\"] = rsi_factor\n",
    "df_all_test[\"rsi_factor_BD\"] = rsi_factor_BD\n",
    "df_all_test[\"rsi_factor_USD\"] = rsi_factor_USD\n",
    "\n",
    "df_all_test[\"rsi_factor\"] = df_all_test[\"rsi_factor\"].fillna(df_all_test[\"rsi_factor\"].mean())\n",
    "df_all_test[\"rsi_factor_BD\"] = df_all_test[\"rsi_factor_BD\"].fillna(df_all_test[\"rsi_factor_BD\"].mean())\n",
    "df_all_test[\"rsi_factor_USD\"] = df_all_test[\"rsi_factor_USD\"].fillna(df_all_test[\"rsi_factor_USD\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add macd features\n",
    "def get_macd(price, slow, fast, smooth):\n",
    "    exp1 = price.ewm(span = fast, adjust = False).mean()\n",
    "    exp2 = price.ewm(span = slow, adjust = False).mean()\n",
    "    macd = pd.DataFrame(exp1 - exp2).rename(columns = {'Close_W':'macd'})\n",
    "    signal = pd.DataFrame(macd.ewm(span = smooth, adjust = False).mean()).rename(columns = {'macd':'signal'})\n",
    "    hist = pd.DataFrame(macd['macd'] - signal['signal']).rename(columns = {0:'hist'})\n",
    "    frames =  [macd, signal, hist]\n",
    "    df = pd.concat(frames, join = 'inner', axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add macd features BD\n",
    "def get_macdBD(price, slow, fast, smooth):\n",
    "    exp1 = price.ewm(span = fast, adjust = False).mean()\n",
    "    exp2 = price.ewm(span = slow, adjust = False).mean()\n",
    "    macd = pd.DataFrame(exp1 - exp2).rename(columns = {'Close_BD':'macdBD'})\n",
    "    signal = pd.DataFrame(macd.ewm(span = smooth, adjust = False).mean()).rename(columns = {'macdBD':'signalBD'})\n",
    "    hist = pd.DataFrame(macd['macdBD'] - signal['signalBD']).rename(columns = {0:'histBD'})\n",
    "    frames =  [macd, signal, hist]\n",
    "    df = pd.concat(frames, join = 'inner', axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add macd features USD\n",
    "def get_macdUSD(price, slow, fast, smooth):\n",
    "    exp1 = price.ewm(span = fast, adjust = False).mean()\n",
    "    exp2 = price.ewm(span = slow, adjust = False).mean()\n",
    "    macd = pd.DataFrame(exp1 - exp2).rename(columns = {'CLOSE':'macdUSD'})\n",
    "    signal = pd.DataFrame(macd.ewm(span = smooth, adjust = False).mean()).rename(columns = {'macdUSD':'signalUSD'})\n",
    "    hist = pd.DataFrame(macd['macdUSD'] - signal['signalUSD']).rename(columns = {0:'histUSD'})\n",
    "    frames =  [macd, signal, hist]\n",
    "    df = pd.concat(frames, join = 'inner', axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "macd_w = get_macd(df_all[\"Close_W\"], 26 , 12 , 9)\n",
    "macd_BD = get_macdBD(df_all[\"Close_BD\"], 26 , 12 , 9)\n",
    "macd_USD = get_macdUSD(df_all[\"CLOSE\"], 26 , 12 , 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = [df_all , macd_w , macd_BD , macd_USD]\n",
    "df_all = pd.concat(df_all , axis=1 , join = \"inner\" , copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "macd_w = get_macd(df_all_test[\"Close_W\"], 26 , 12 , 9)\n",
    "macd_BD = get_macdBD(df_all_test[\"Close_BD\"], 26 , 12 , 9)\n",
    "macd_USD = get_macdUSD(df_all_test[\"CLOSE\"], 26 , 12 , 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_test = [df_all , macd_w , macd_BD , macd_USD]\n",
    "df_all_test = pd.concat(df_all_test , axis=1 , join = \"inner\" , copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the types (objects)\n",
    "df_all[\"TP\"] = pd.to_numeric(df_all[\"TP\"])\n",
    "df_all[\"predict_next\"] = pd.to_numeric(df_all[\"predict_next\"])\n",
    "df_all[\"Open_W\"] = pd.to_numeric(df_all[\"Open_W\"])\n",
    "df_all[\"High_W\"] = pd.to_numeric(df_all[\"High_W\"])\n",
    "df_all[\"Low_W\"] = pd.to_numeric(df_all[\"Low_W\"])\n",
    "df_all[\"Close_W\"] = pd.to_numeric(df_all[\"Close_W\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the types (objects)\n",
    "df_all_test[\"TP\"] = pd.to_numeric(df_all_test[\"TP\"])\n",
    "df_all_test[\"predict_next\"] = pd.to_numeric(df_all_test[\"predict_next\"])\n",
    "df_all_test[\"Open_W\"] = pd.to_numeric(df_all_test[\"Open_W\"])\n",
    "df_all_test[\"High_W\"] = pd.to_numeric(df_all_test[\"High_W\"])\n",
    "df_all_test[\"Low_W\"] = pd.to_numeric(df_all_test[\"Low_W\"])\n",
    "df_all_test[\"Close_W\"] = pd.to_numeric(df_all_test[\"Close_W\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "features = df_all.drop(\"predict_next\" , axis = 1)\n",
    "target = df_all[\"predict_next\"]\n",
    "features_time = features[\"Datetime\"]\n",
    "features.drop(\"Datetime\" , axis = 1 , inplace = True)\n",
    "features_time = pd.DataFrame(features_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = df_all_test\n",
    "features_time_test = features_test[\"Datetime\"]\n",
    "features_test.drop(\"Datetime\" , axis = 1 , inplace = True)\n",
    "features_time_test = pd.DataFrame(features_time_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_scaled = StandardScaler().fit_transform(features)\n",
    "features_scaled = pd.DataFrame(features_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_scaled_test = StandardScaler().fit_transform(features_test)\n",
    "features_scaled_test = pd.DataFrame(features_scaled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\.conda\\envs\\myenv\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "c:\\Users\\Admin\\.conda\\envs\\myenv\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(features_scaled , target)\n",
    "cumsum_var = np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=19).fit_transform(features_scaled)\n",
    "pca_df = pd.DataFrame(pca  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_test = PCA()\n",
    "pca_test.fit(features_scaled_test)\n",
    "cumsum_var = np.cumsum(pca_test.explained_variance_ratio_)\n",
    "pca_test = PCA(n_components=19).fit_transform(features_scaled_test)\n",
    "pca_df_test = pd.DataFrame(pca_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\.conda\\envs\\myenv\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(init= \"random\" ,n_clusters=5,n_init=10,max_iter=300,random_state=42)\n",
    "kmeans.fit(pca_df)\n",
    "labels = pd.DataFrame( kmeans.labels_ , columns={\"kmeans\"})\n",
    "pca_df = [pca_df , labels]\n",
    "pca_df = pd.concat(pca_df , axis = 1)\n",
    "pca_df[\"kmeans\"] = pca_df[\"kmeans\"].astype(\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(init= \"random\" ,n_clusters=5,n_init=10,max_iter=300,random_state=42)\n",
    "kmeans.fit(pca_df_test)\n",
    "labels_test = pd.DataFrame( kmeans.labels_ , columns={\"kmeans\"})\n",
    "pca_df_test = [pca_df_test , labels_test]\n",
    "pca_df_test = pd.concat(pca_df_test , axis = 1)\n",
    "pca_df_test[\"kmeans\"] = pca_df_test[\"kmeans\"].astype(\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = pca_df.iloc[ 0:50000,:]\n",
    "\n",
    "features_test = pca_df.iloc[50000: , :]\n",
    "\n",
    "target_train = target.iloc[0:50000]\n",
    "target_train = pd.DataFrame(target_train)\n",
    "\n",
    "target_test  = target.iloc[50000:]\n",
    "target_test = pd.DataFrame(target_test)\n",
    "\n",
    "features_train.reset_index(drop=True , inplace = True)\n",
    "target_train.reset_index(drop=True , inplace = True)\n",
    "pca_df = [features_train , target_train]\n",
    "pca_df = pd.concat(pca_df , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\.conda\\envs\\myenv\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "params_grid = {\n",
    "    'n_estimators' : [10 , 25 ] ,\n",
    "    'max_depth': [3 ,  5],\n",
    "    'max_leaf_nodes' : [5, 15 ],\n",
    "    'learning_rate' : [ 0.4 , 0.7 ] \n",
    "}\n",
    "clfG = GradientBoostingClassifier()\n",
    "grid_search = GridSearchCV(estimator= clfG, \n",
    "                           param_grid=params_grid, \n",
    "                           cv=5, scoring = \"f1\")\n",
    "grid_search.fit(features_train, target_train.values.ravel())\n",
    "bestclfG = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_best = bestclfG.predict(pca_df_test)\n",
    "pred_best = pd.DataFrame(pred_best)\n",
    "pred_best = [features_time_test , pred_best]\n",
    "pred_best = pd.concat(pred_best , axis= 1 )\n",
    "pred_best = pd.DataFrame(pred_best)\n",
    "pred_best.to_csv(\"pred_best.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-09 09:56:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-09 09:57:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-09 09:58:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-09 09:59:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-09 10:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67201</th>\n",
       "      <td>2022-07-21 14:54:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67202</th>\n",
       "      <td>2022-07-21 14:55:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67203</th>\n",
       "      <td>2022-07-21 14:56:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67204</th>\n",
       "      <td>2022-07-21 14:57:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67205</th>\n",
       "      <td>2022-07-21 14:58:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67206 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Datetime  0\n",
       "0      2022-01-09 09:56:00  0\n",
       "1      2022-01-09 09:57:00  1\n",
       "2      2022-01-09 09:58:00  1\n",
       "3      2022-01-09 09:59:00  0\n",
       "4      2022-01-09 10:00:00  0\n",
       "...                    ... ..\n",
       "67201  2022-07-21 14:54:00  0\n",
       "67202  2022-07-21 14:55:00  0\n",
       "67203  2022-07-21 14:56:00  0\n",
       "67204  2022-07-21 14:57:00  0\n",
       "67205  2022-07-21 14:58:00  0\n",
       "\n",
       "[67206 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.75      0.72     44060\n",
      "           1       0.41      0.32      0.36     23146\n",
      "\n",
      "    accuracy                           0.61     67206\n",
      "   macro avg       0.55      0.54      0.54     67206\n",
      "weighted avg       0.59      0.61      0.59     67206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_all_test.predict_next , pred_best[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        1\n",
       "2        1\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "67201    0\n",
       "67202    0\n",
       "67203    0\n",
       "67204    0\n",
       "67205    0\n",
       "Name: 0, Length: 67206, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_best[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28cbdfba20cf98beee3bca377205d83816279904897bace16e8c94489c68ffae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
